{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heesukjang/W207_AppliedML_Fall2022/blob/main/11_18_KerasTuner_plots_Final_HyperparamTuning_ImageAug_CM_CR_ROCcurve_IDC_Prediction_heesuk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FALL 2022<br>\n",
        "W207 Applied Machine Learning<br>\n",
        "Heesuk Jang\n",
        " \n",
        "\n",
        "**XGBoost:** an optimized version of Gradient Boosting / much more evolved version of Random Forest in terms of speed and accuracy\n",
        "\n",
        "#Predicting IDC with Breast Histopathology Images using CNN\n",
        "\n"
      ],
      "metadata": {
        "id": "5DebDWCL0KeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRkZHKoWswZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33339e14-86e5-4d3d-babc-77ba09a18694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import joblib\n",
        "import glob\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import *                            # confusion_matrix, log_loss, accuracy_score\n",
        "from sklearn.model_selection import *                    # train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import *  \n",
        "# from sklearn.ensemble import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.linear_model import *                       # LinearRegression\n",
        "from sklearn.discriminant_analysis import *\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import metrics\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import RandomFlip, RandomZoom, RandomRotation, Conv2D, MaxPooling2D, AveragePooling2D, Input, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, RMSprop\n",
        "from keras.layers import ReLU, LeakyReLU\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import ReLU, LeakyReLU\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, auc\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "tf.get_logger().setLevel('INFO')\n",
        "\n",
        "import cv2 as cv\n",
        "import skimage.io as io\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Required to read the data from Kaggle\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Optuna and Version Check"
      ],
      "metadata": {
        "id": "2NlEwp6jmZsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet optuna\n",
        "# import optuna\n",
        "# optuna.__version__"
      ],
      "metadata": {
        "id": "vbF2pFtlbJaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "# !pip install xgboost\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "XmhJR4rTcSlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73269dd-2f3e-4a3a-acf5-be3ef5a0750e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 36.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the GPU"
      ],
      "metadata": {
        "id": "2JC1sfIYmRuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "lpo1_3kNlzoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the TPU"
      ],
      "metadata": {
        "id": "s3LsW0cGnF8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU"
      ],
      "metadata": {
        "id": "Xqgm8yAcmT7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip gdrive/MyDrive/Kaggle/CNN_IDC/Dataset.zip\n",
        "\n",
        "#replace these paths with the paths of your \n",
        "val_image_directory = '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Validate'\n",
        "train_image_directory = '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Train'\n",
        "test_image_directory = '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Test'\n",
        "directory_path = '/content/gdrive/MyDrive/Kaggle/CNN_IDC'"
      ],
      "metadata": {
        "id": "uyWJuOkZuCVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paths(directory):\n",
        "  all_path = []\n",
        "  idc_image_path = []\n",
        "  idc_image_label = []\n",
        "\n",
        "  for dir, subdir, files in os.walk(directory):\n",
        "    path = dir + \"/\"\n",
        "    all_path.append(path)\n",
        "\n",
        "  for i in range(len(all_path)):\n",
        "    for file in os.listdir(all_path[i]):\n",
        "      test = file\n",
        "      path = all_path[i] + test\n",
        "      if path.lower().endswith('.png'):\n",
        "        idc_image_path.append(path)\n",
        "\n",
        "  for i in range(len(idc_image_path)):\n",
        "    split_test = idc_image_path[i]\n",
        "    split_path = split_test.split(\"/\")\n",
        "    directory_name = split_path[7]\n",
        "    idc_image_label.append('class_' + split_path[8])\n",
        "  return idc_image_path, idc_image_label, directory_name"
      ],
      "metadata": {
        "id": "N892xh1IM4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, train_labels, train_dir = get_paths(train_image_directory)\n",
        "val_paths, val_labels, val_dir = get_paths(val_image_directory)\n",
        "test_paths, test_labels, test_dir = get_paths(test_image_directory)"
      ],
      "metadata": {
        "id": "SJ6Cl4wtmxjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[:5])\n",
        "print(train_labels[-5:])\n",
        "\n",
        "print(len(train_paths), len(train_labels))\n",
        "print(len(test_paths), len(test_labels))\n",
        "print(len(val_paths), len(val_labels))"
      ],
      "metadata": {
        "id": "NIf9ETAsmxa2",
        "outputId": "7f1b4e60-60bf-4d80-be16-eed19b28702f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['class_0', 'class_0', 'class_0', 'class_0', 'class_0']\n",
            "['class_1', 'class_1', 'class_1', 'class_1', 'class_1']\n",
            "800 800\n",
            "200 200\n",
            "200 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_paths[:2])\n",
        "print(train_labels[:10])\n",
        "print(train_dir)"
      ],
      "metadata": {
        "id": "uCJzEwS8mxSR",
        "outputId": "202fa1ae-8a3f-4769-b587-1c1ad901aefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Train/0/12880_idx5_x451_y701_class0.png', '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Train/0/9345_idx5_x2001_y2001_class0.png']\n",
            "['class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0']\n",
            "Train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataframes(idc_image_path, idc_image_label, directory_name):\n",
        "  same_name = directory_name.lower() + '_'\n",
        "  #creating the dataframes that we will be passing to our generators\n",
        "  idc_data_cleaned = {'path': idc_image_path,\n",
        "            'label': idc_image_label}\n",
        "  idc_df = pd.DataFrame(idc_data_cleaned)\n",
        "  idc_df['label_int'] = idc_df['label'].str.split(\"_\", expand=True)[1]    # Added a new column 'label_int'\n",
        "  df = idc_df.sample(frac = 1)\n",
        "  print(df)\n",
        "  csv_path = directory_path\n",
        "  csv_file = df.to_csv(csv_path + '/' + same_name + 'idc_dataframe.csv')\n",
        "  csv_file_path = csv_path + '/' + same_name + 'idc_dataframe.csv'\n",
        "  return csv_file_path"
      ],
      "metadata": {
        "id": "_WU9qt2RmxHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = create_dataframes(train_paths, train_labels, train_dir)\n",
        "print('type(train_dataframe): ',type(train_dataframe))\n",
        "train_dataframe"
      ],
      "metadata": {
        "id": "MJxUgm39NeU7",
        "outputId": "851e71dd-bd03-411f-cf75-2d8f12cfff5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  path    label label_int\n",
            "534  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "598  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "258  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "505  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "131  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "480  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "374  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "718  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "496  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "657  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "\n",
            "[800 rows x 3 columns]\n",
            "type(train_dataframe):  <class 'str'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Kaggle/CNN_IDC/train_idc_dataframe.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = create_dataframes(train_paths, train_labels, train_dir)\n",
        "train_generator = pd.read_csv(train_dataframe)\n",
        "\n",
        "test_dataframe = create_dataframes(test_paths, test_labels, test_dir)\n",
        "test_generator = pd.read_csv(test_dataframe)\n",
        "\n",
        "val_dataframe = create_dataframes(val_paths, val_labels, val_dir)\n",
        "val_generator = pd.read_csv(val_dataframe)"
      ],
      "metadata": {
        "id": "hnDu_3N4mw1G",
        "outputId": "4bca811f-7554-4f7a-e8ad-9d6c65f321d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  path    label label_int\n",
            "542  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "706  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "729  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "241  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "205  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "425  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "208  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "191  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "128  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "225  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "\n",
            "[800 rows x 3 columns]\n",
            "                                                  path    label label_int\n",
            "6    /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "117  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "123  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "34   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "48   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "180  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "171  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "26   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "192  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "89   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "\n",
            "[200 rows x 3 columns]\n",
            "                                                  path    label label_int\n",
            "154  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "152  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "115  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "166  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "86   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "43   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "189  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "31   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "7    /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "169  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "\n",
            "[200 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # import tqdm\n",
        "\n",
        "# Apply gray scale to all images, flatten and store array / shape in new columns\n",
        "def get_img_arrays(df,):\n",
        "    # read each image array from corresponding path as grayscale and flatten the image array\n",
        "    df['img_array'] = df.progress_apply(lambda x : io.imread(x['path'],as_gray=True).flatten(),axis=1) # make sure to specify axis = 1\n",
        "    # get the shape of each image array and store it in the dataframe\n",
        "    df['array_shape'] = df.progress_apply(lambda x : x['img_array'].shape[0],axis=1) # make sure to specify axis = 1\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "nkWYjNvnHrrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # import tqdm\n",
        "tqdm.pandas() # initialize tqdm for pandas\n",
        "\n",
        "# # tqdm is a library that enables you to visualize the progress of a for loop by displaying a configurable progress bar\n",
        "\n",
        "train_generator = get_img_arrays(df = train_generator)\n",
        "val_generator = get_img_arrays(df = val_generator)\n",
        "test_generator = get_img_arrays(df = test_generator)"
      ],
      "metadata": {
        "id": "d9iPVxRZHreQ",
        "outputId": "3a6a281c-d899-47a6-e6f6-0475bfb084f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 670/800 [04:40<00:54,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-16d6a4c8c26d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# # tqdm is a library that enables you to visualize the progress of a for loop by displaying a configurable progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d1f6dadd4eb3>\u001b[0m in \u001b[0;36mget_img_arrays\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_img_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# read each image array from corresponding path as grayscale and flatten the image array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_array'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_gray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make sure to specify axis = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get the shape of each image array and store it in the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'array_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make sure to specify axis = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d1f6dadd4eb3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_img_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# read each image array from corresponding path as grayscale and flatten the image array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_array'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_gray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make sure to specify axis = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get the shape of each image array and store it in the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'array_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make sure to specify axis = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                                (plugin, kind))\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_read_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mmodename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODENAMES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/format.py\u001b[0m in \u001b[0;36msearch_read_format\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# Select the first that can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_formats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mcan_read\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mformat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mread\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcan_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/plugins/pillow.py\u001b[0m in \u001b[0;36m_can_read\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPEN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugin_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstbytes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36mfirstbytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \"\"\"\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_firstbytes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_first_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_firstbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_read_first_bytes\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# Read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_firstbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_n_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Set back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36mread_n_bytes\u001b[0;34m(f, N)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mextra_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextra_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_generator.array_shape.value_counts())\n",
        "# print(val_generator.array_shape.value_counts())\n",
        "# print(test_generator.array_shape.value_counts())"
      ],
      "metadata": {
        "id": "wAv7RXmVQX4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop these images as they add unnecessary noise to our model\n",
        "train_weird_imgs = train_generator[train_generator['array_shape'] != 2500]\n",
        "val_weird_imgs = val_generator[val_generator['array_shape'] != 2500]\n",
        "test_weird_imgs = test_generator[test_generator['array_shape'] != 2500]\n",
        "\n",
        "\n",
        "weird_imgs = train_weird_imgs.append(val_weird_imgs)\n",
        "weird_imgs = weird_imgs.append(test_weird_imgs)\n",
        "weird_imgs['dataset'] = weird_imgs['path'].str.split('/', expand=True)[7]\n",
        "weird_imgs.reset_index(drop=True)\n",
        "\n",
        "train_generator.drop(train_weird_imgs.index,inplace=True)\n",
        "val_generator.drop(val_weird_imgs.index,inplace=True)\n",
        "test_generator.drop(test_weird_imgs.index,inplace=True)\n",
        "\n",
        "# print(len(weird_imgs))\n",
        "# print(len(train_generator))\n",
        "# print(len(val_generator))\n",
        "# print(len(test_generator))\n",
        "# print(train_generator.columns)\n",
        "# val_generator.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gpOS9_alHrTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import skimage.io as io\n",
        "\n",
        "def display_images(subclass):\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20,8))\n",
        "  for idx, ax in enumerate(axes.flat):\n",
        "    image_wo_path = os.path.basename(subclass.path[idx])\n",
        "    # print(image_wo_path)\n",
        "    subtitle = 'Class ' + image_wo_path.rsplit('.')[0][-1] + ': ' + subclass.dataset[idx]\n",
        "    img = io.imread(subclass.path[idx])\n",
        "    ax.imshow(img)\n",
        "    # ax.axis('off')\n",
        "    ax.set_title(subtitle, size=14)   \n",
        "  fig.tight_layout() \n",
        "  plt.show() \n",
        "\n",
        "print()\n",
        "display_images(weird_imgs.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "x9euSsBNHrIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data using ImageDataGenerator"
      ],
      "metadata": {
        "id": "iHdf3Jd0ipmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://faroit.com/keras-docs/0.3.3/preprocessing/image/\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "\n",
        "def data_generator():\n",
        "\n",
        "  with_aug_datagen = ImageDataGenerator(\n",
        "      featurewise_center = True,                     # transforms the images to 0 mean\n",
        "      featurewise_std_normalization = True,          # divide inputs by std of the dataset\n",
        "      # zca_epsilon=1e-06,\n",
        "      rotation_range=30,                      # randomly rotate image by 10 degrees\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      brightness_range=[0.4, 1.5],\n",
        "      shear_range=0.2,                        # distort image along an axis mostly to create or recify the perception angles     \n",
        "      zoom_range=0.2,                         # zomming image: zoom_range > 1 => zoom out, zoom_range < 1 => zoom in                  \n",
        "      fill_mode='nearest',                    # when the image is rotated, some pixels will move outside the image and leave an empty area that needs to be filled in, 'nearest': simply replace the empty area with the nearest spectral values.\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True)\n",
        "      \n",
        "  # without_aug_datagen = ImageDataGenerator()\n",
        "  without_aug_datagen = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)   # False is default so need to set to True\n",
        "  \n",
        "# =================================================================\n",
        "  # if with_augmented_images:\n",
        "  # train_data_generator = with_aug_datagen.flow_from_dataframe(\n",
        "  #       train_generator,\n",
        "  #       directory = None,\n",
        "  #       x_col =  'path',\n",
        "  #       y_col =  'label',\n",
        "  #       weight_col=None,\n",
        "  #       target_size=(50,50),\n",
        "  #       color_mode=\"grayscale\",\n",
        "  #       class_mode=\"categorical\",\n",
        "  #       batch_size=32,\n",
        "  #       shuffle=True,\n",
        "  #       seed=1234\n",
        "  #       # validate_filenames=True\n",
        "  #   )\n",
        "  # else:\n",
        "  train_data_generator = without_aug_datagen.flow_from_dataframe(\n",
        "      train_generator,\n",
        "      directory = None,\n",
        "      x_col =  'path',\n",
        "      y_col =  'label',\n",
        "      weight_col=None,\n",
        "      target_size=(50,50),\n",
        "      color_mode=\"grayscale\",\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=32,\n",
        "      shuffle=True,\n",
        "      seed=1234\n",
        "      # validate_filenames=True\n",
        "  )\n",
        "\n",
        "  validation_data_generator = without_aug_datagen.flow_from_dataframe(\n",
        "      val_generator,\n",
        "      directory = None,\n",
        "      x_col =  'path',\n",
        "      y_col =  'label',\n",
        "      weight_col=None,\n",
        "      # target_size=(hp_target_size, hp_target_size),\n",
        "      target_size=(50,50),\n",
        "      color_mode=\"grayscale\",\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=32,\n",
        "      shuffle=True,\n",
        "      seed=1234\n",
        "      # validate_filenames=True\n",
        "  )\n",
        "\n",
        "  test_data_generator = without_aug_datagen.flow_from_dataframe(\n",
        "      test_generator,\n",
        "      directory = None,\n",
        "      x_col =  'path',\n",
        "      y_col =  'label',\n",
        "      weight_col=None,\n",
        "      target_size=(50,50),\n",
        "      color_mode=\"grayscale\",\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=32,\n",
        "      shuffle=True,              # Kesha set to shuffle=True but we don't want to shuffle our testing data around, which it does so by default\n",
        "      seed=1234\n",
        "      # validate_filenames=True\n",
        "  )\n",
        "  return train_data_generator, validation_data_generator, test_data_generator\n",
        "\n",
        "# train_data_generator, validation_data_generator, test_data_generator = data_generator(with_augmented_images=True)\n",
        "train_data_generator, validation_data_generator, test_data_generator = data_generator()"
      ],
      "metadata": {
        "id": "9_IibHAwxE6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build CNN Model and Hyperparameter Tuning\n",
        "- **ReduceLROnPlateau**: A scheduling technique that monitors a particular quantity and decays the learning rate when the quantity is stop improving.\n",
        "- **ModelCheckpoint**: A sch\n",
        "- **BatchNormalization**: A feature that we add between the layers of neural network and it continuously takes the output from the previous layer and normalizes it before sending it to the next layer thereby helping stablizing the NN"
      ],
      "metadata": {
        "id": "I19l1YOYsfOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img_height = 50\n",
        "# img_width = 50\n",
        "# img_channel = 1\n",
        "# input_shape = (img_height, img_width, img_channel)"
      ],
      "metadata": {
        "id": "OQcHdDdYJnol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.layers import ReLU, LeakyReLU\n",
        "# # https://keras.io/guides/keras_tuner/getting_started/\n",
        "\n",
        "# def build_cnn(hp): \n",
        "#   # train_data_generator, validation_data_generator, test_data_generator = data_generator(hp)\n",
        "\n",
        "#   tf.keras.backend.clear_session()\n",
        "#   tf.random.set_seed(0)\n",
        "\n",
        "# # Define ranges of hyperparam values \n",
        "#   # hp_img_size = hp.get('hp_target_size')\n",
        "#   # hp_img_size = hp.Int('img_size', 50,96,120)\n",
        "#   # hp_img_size = hp.Int('img_size', 50,96)\n",
        "#   hp_kernel_size = hp.Int('kernel_size', min_value=2, max_value=5, step=1)\n",
        "#   hp_strides = hp.Int('strides', min_value=1, max_value=2, step=1)\n",
        "#   hp_pool_size = hp.Int('pool_size', min_value=2, max_value=3, step=1)\n",
        "#   hp_activation = hp.Choice('activation', values=['tanh', 'relu', 'softmax', 'sigmoid', 'leaky_relu', 'elu', 'gelu','selu','swish'], default='relu')\n",
        "#   hp_optimizer = hp.Choice('optimizer', values=['adadelta', 'adagrad', 'adam', 'rmsprop', 'sgd'], default='adam')\n",
        "#   hp_filters_1 = hp.Int('filters_1', min_value=8, max_value=64, step=8)\n",
        "#   hp_filters_2 = hp.Int('filters_2', min_value=8, max_value=128, step=16)\n",
        "#   hp_filters_3 = hp.Int('filters_3', min_value=32, max_value=128, step=16)\n",
        "#   hp_dense = hp.Int('dense_units', min_value=32, max_value=512, step=34)\n",
        "#   hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-2)\n",
        "#   hp_dropout_1 = hp.Float('dropout_rate_1', min_value=0.1, max_value=0.5, default=0.1, step=0.1)\n",
        "#   hp_dropout_2 = hp.Float('dropout_rate_2', min_value=0.3, max_value=0.5, default=0.25, step=0.1)\n",
        "#   hp_dropout_3 = hp.Float('dropout_rate_3', min_value=0.3, max_value=0.5, default=0.35, step=0.1)\n",
        "#   hp_reduction_type = hp.Choice('reduction_type', values=['global_avg_pooling2d', 'max_pooling2d'])\n",
        "\n",
        "#   # Define the model\n",
        "#   model = tf.keras.Sequential()\n",
        "#   # print('input_shape: ', (hp_img_size, hp_img_size, 1))\n",
        "#   # 1st set of neural network layers (Input layer)\n",
        "#   model.add(Conv2D(filters=hp_filters_1, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation, input_shape = (50,50,1)))  \n",
        "#   # model.add(Conv2D(filters=hp_filters_1, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation, input_shape = (hp_img_size, hp_img_size, 1)))  \n",
        "#   if hp_reduction_type == 'global_avg_pooling2d':\n",
        "#     model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "#   else:\n",
        "#     model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides,hp_strides)))  \n",
        "#   # if hp.Boolean('dropout_1'):\n",
        "#     # model.add(Dropout(hp_dropout_1))\n",
        "  \n",
        "#   # 2nd set of neural network layers\n",
        "#   # if hp.Boolean('conv_layer_2'):  \n",
        "#   model.add(Conv2D(filters=hp_filters_2, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation))\n",
        "#   # if hp.Boolean('BatchNormalization_1'):\n",
        "#   model.add(BatchNormalization())\n",
        "#   if hp_reduction_type == 'global_avg_pooling2d':\n",
        "#     model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "#   else:\n",
        "#     model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides,hp_strides)))  \n",
        "#   # if hp.Boolean('dropout_2'):\n",
        "#     # model.add(Dropout(hp_dropout_1))\n",
        "\n",
        "#   # 3rd set of neural network layers\n",
        "#   # if hp.Boolean('conv_layer_3'):  \n",
        "#   model.add(Conv2D(filters=hp_filters_3, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation.lower()))\n",
        "#   # if hp.Boolean('BatchNormalization_2'):\n",
        "#   model.add(BatchNormalization())\n",
        "#   if hp_reduction_type == 'global_avg_pooling2d':\n",
        "#     model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "#   else:\n",
        "#     model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides, hp_strides)))  \n",
        "#   # if hp.Boolean('dropout_3'):\n",
        "#   # model.add(Dropout(hp_dropout_2))\n",
        "\n",
        "#   # Flatten layer\n",
        "#   model.add(Flatten())\n",
        "\n",
        "#   # Fully connected dense layer\n",
        "#   model.add(Dense(units = hp_dense, activation = hp_activation))\n",
        "#   # if hp.Boolean('BatchNormalization_3'):\n",
        "#   model.add(BatchNormalization())\n",
        "#   if hp.Boolean('dropout_4'):\n",
        "#     model.add(Dropout(hp_dropout_3))\n",
        "  \n",
        "#   # Output layer\n",
        "#   model.add(Dense(units = 2, activation = 'softmax'))        # output layer\n",
        "  \n",
        "#   # Define the optimizer\n",
        "#   def selected_optimizer(optimizer):\n",
        "#     if optimizer.lower() == 'sgd':\n",
        "#         return SGD(learning_rate=hp_learning_rate)           # SGD(learning_rate=learning_rate, momentum=0.95, decay=1, nesterov=True)\n",
        "#     elif optimizer.lower() == 'adam':\n",
        "#         return Adam(learning_rate=hp_learning_rate)          # Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8)\n",
        "#     elif optimizer.lower() == 'adadelta':\n",
        "#         return Adadelta(learning_rate=hp_learning_rate)      # Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=1e-6)\n",
        "#     elif optimizer.lower() == 'adagrad':\n",
        "#         return Adagrad(learning_rate=hp_learning_rate)       # Adagrad(learning_rate=learning_rate, epsilon=1e-6)\n",
        "#     elif optimizer.lower() == 'rmsprop':\n",
        "#         return RMSprop(learning_rate=hp_learning_rate)       # RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-6)\n",
        "\n",
        "#   # Compile the model\n",
        "#   model.compile(loss=CategoricalCrossentropy(), \n",
        "#                 optimizer=selected_optimizer(hp_optimizer), \n",
        "#                 metrics=['accuracy'])\n",
        "#   return model"
      ],
      "metadata": {
        "id": "WEiCXMc-cRJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import ReLU, LeakyReLU\n",
        "# https://keras.io/guides/keras_tuner/getting_started/\n",
        "\n",
        "def build_cnn(hp): \n",
        "  # train_data_generator, validation_data_generator, test_data_generator = data_generator(hp)\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "# Define ranges of hyperparam values \n",
        "  # hp_img_size = hp.get('hp_target_size')\n",
        "  # hp_img_size = hp.Int('img_size', 50,96,120)\n",
        "  # hp_img_size = hp.Int('img_size', 50,96)\n",
        "  hp_kernel_size = hp.Int('kernel_size', min_value=2, max_value=5, step=1)\n",
        "  hp_strides = hp.Int('strides', min_value=1, max_value=2, step=1)\n",
        "  hp_pool_size = hp.Int('pool_size', min_value=2, max_value=3, step=1)\n",
        "  hp_activation = hp.Choice('activation', values=['relu', 'leaky_relu', 'elu', 'gelu','selu'], default='relu')\n",
        "  hp_optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'], default='adam')\n",
        "  hp_filters_1 = hp.Int('filters_1', min_value=8, max_value=64, step=8)\n",
        "  # hp_filters_2 = hp.Int('filters_2', min_value=8, max_value=128, step=16)\n",
        "  # hp_filters_3 = hp.Int('filters_3', min_value=32, max_value=128, step=16)\n",
        "  hp_dense = hp.Int('dense_units', min_value=32, max_value=256, step=34)\n",
        "  hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-2)\n",
        "  hp_dropout = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, default=0.1, step=0.1)\n",
        "  # hp_dropout_2 = hp.Float('dropout_rate_2', min_value=0.3, max_value=0.5, default=0.25, step=0.1)\n",
        "  # hp_dropout_3 = hp.Float('dropout_rate_3', min_value=0.3, max_value=0.5, default=0.35, step=0.1)\n",
        "  hp_reduction_type = hp.Choice('reduction_type', values=['global_avg_pooling2d', 'max_pooling2d'])\n",
        "\n",
        "  # Define the model\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Input layer\n",
        "  model.add(Conv2D(hp.Int('input_units', min_value=16, max_value=128, step=16), kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation, input_shape = (50,50,1)))  \n",
        "  if hp_reduction_type == 'global_avg_pooling2d':\n",
        "    model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "  else:\n",
        "    model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides,hp_strides)))  \n",
        "\n",
        "  \n",
        "  # 2nd set of neural network layers\n",
        "  # if hp.Boolean('conv_layer_2'):  \n",
        "  for i in range(hp.Int('n_layers', 1, 3))\n",
        "    model.add(Conv2D(hp.Int(f'conv2d_{i}_units', min_value=32, max_value=128, step=32), kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation))\n",
        "    if hp_reduction_type == 'global_avg_pooling2d':\n",
        "      model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "    else:\n",
        "      model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides,hp_strides)))  \n",
        "    model.add(BatchNormalization())\n",
        "    if hp.Boolean('dropout'):\n",
        "      model.add(Dropout(hp_dropout))\n",
        "\n",
        "  # # 3rd set of neural network layers\n",
        "  # # if hp.Boolean('conv_layer_3'):  \n",
        "  # model.add(Conv2D(filters=hp_filters_3, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation.lower()))\n",
        "  # # if hp.Boolean('BatchNormalization_2'):\n",
        "  # model.add(BatchNormalization())\n",
        "  # if hp_reduction_type == 'global_avg_pooling2d':\n",
        "  #   model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "  # else:\n",
        "  #   model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides, hp_strides)))  \n",
        "  # # if hp.Boolean('dropout_3'):\n",
        "  # # model.add(Dropout(hp_dropout_2))\n",
        "\n",
        "  # Flatten layer\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Fully connected dense layer\n",
        "  model.add(Dense(units = hp_dense, activation = hp_activation))\n",
        "  # model.add(BatchNormalization())\n",
        "  # if hp.Boolean('dropout_4'):\n",
        "  model.add(Dropout(hp_dropout))\n",
        "  \n",
        "  # Output layer\n",
        "  model.add(Dense(units = 2, activation = 'softmax'))        # output layer\n",
        "  \n",
        "  # Define the optimizer\n",
        "  def selected_optimizer(optimizer):\n",
        "    if optimizer.lower() == 'sgd':\n",
        "        return SGD(learning_rate=hp_learning_rate)           # SGD(learning_rate=learning_rate, momentum=0.95, decay=1, nesterov=True)\n",
        "    elif optimizer.lower() == 'adam':\n",
        "        return Adam(learning_rate=hp_learning_rate)          # Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8)\n",
        "    # elif optimizer.lower() == 'adadelta':\n",
        "    #     return Adadelta(learning_rate=hp_learning_rate)      # Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=1e-6)\n",
        "    # elif optimizer.lower() == 'adagrad':\n",
        "    #     return Adagrad(learning_rate=hp_learning_rate)       # Adagrad(learning_rate=learning_rate, epsilon=1e-6)\n",
        "    elif optimizer.lower() == 'rmsprop':\n",
        "        return RMSprop(learning_rate=hp_learning_rate)       # RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-6)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=CategoricalCrossentropy(), \n",
        "                optimizer=selected_optimizer(hp_optimizer), \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "-b-MQQn9Erwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the **max_trials** variable represents the number of hyperparameter combinations that will be tested by the tuner, while the **execution_per_trial** variable is the number of models that should be built and fit for each trial for robustness purposes. The next section explains how to set them\n",
        "\n",
        "https://www.sicara.fr/blog-technique/hyperparameter-tuning-keras-tuner<br>\n",
        "https://www.kaggle.com/code/fchollet/keras-kerastuner-best-practices/notebook"
      ],
      "metadata": {
        "id": "SnHwzogJu0BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "stop_early = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 3)\n",
        "# stop_early = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 3)\n",
        "# 1) Hyperband\n",
        "# tuner = kt.Hyperband(build_cnn,                     \n",
        "#                      objective='val_accuracy',\n",
        "#                      max_epochs=10,\n",
        "#                      factor=3,\n",
        "#                      overwrite=True,\n",
        "#                      directory='hj_dir',\n",
        "#                      project_name='breast_cancer_classification')\n",
        "# 2) RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "          # CNN_HyperModel(),\n",
        "          build_cnn,\n",
        "          objective=\"val_accuracy\",\n",
        "          max_trials=10,                     # max_trials=50 or 10,\n",
        "          executions_per_trial=2,           # executions_per_trial=10,\n",
        "          overwrite=True,\n",
        "          directory=\"hj_dir\",\n",
        "          project_name=\"breast_cancer_classification\")\n",
        "\n",
        "tuner.search(train_data_generator, \n",
        "             epochs=10, \n",
        "             callbacks=[stop_early], \n",
        "             validation_data=validation_data_generator)"
      ],
      "metadata": {
        "id": "yH0jfZqX2Qxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_all = np.concatenate((train_data_generator, validation_data_generator))"
      ],
      "metadata": {
        "id": "4-ChFsRGFKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
      ],
      "metadata": {
        "id": "5L2ndY3cZWSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]     # num_trials=5\n",
        "# model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters()[0]     # num_trials=5\n",
        "model = tuner.get_best_models()[0]\n",
        "# model = build_cnn(best_hps)\n",
        "\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
        "# from sklearn import svm\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.0001, patience=1, verbose=1)\n",
        "\n",
        "file_path = 'weights.hdf5'    # save the weights and biases\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Fit with the entire dataset (both training and validation sets)\n",
        "\n",
        "history = model.fit(train_data_generator,\n",
        "                  epochs=10,\n",
        "                  # steps_per_epoch=len(train_data_generator)//BATCH_SIZE, \n",
        "                  callbacks=[lr_reduce, checkpoint, stop_early],\n",
        "                  validation_data = validation_data_generator)\n",
        "                  # validation_steps=len(validation_data_generator)//BATCH_SIZE)\n",
        "# history = model.fit_generator(train_data_generator.flow(train_data_generator, batch_size=BATCH_SIZE),\n",
        "#                   steps_per_epoch = train_generator.shape[0] / BATCH_SIZE,\n",
        "#                   epochs=60,\n",
        "#                   callbacks=[lr_reduce, checkpoint, stop_early],\n",
        "#                   validation_data = validation_data_generator)\n",
        "# history = model.fit_generator(train_data_generator,\n",
        "#                   epochs=60,\n",
        "#                   callbacks=[lr_reduce, checkpoint, stop_early],\n",
        "#                   validation_data = validation_data_generator).decision_function(test_data_generator)\n",
        "# pd.DataFrame(history.history)\n",
        "\n",
        "# Plot loss and accuracy of best model on every epoch\n",
        "hist = history.history\n",
        "x_arr = np.arange(len(hist['loss'])) + 1\n",
        "\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "ax.plot(x_arr, hist['loss'], '-o', label='Train Loss')\n",
        "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation Loss')\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_xlabel('Epoch', size=14)\n",
        "ax.set_ylabel('Loss', size=14)\n",
        "ax.set_title('Loss', size=20)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "ax.plot(x_arr, hist['accuracy'], '-o', label='Train Acc.')\n",
        "ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation Acc.')\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_xlabel('Epoch', size=14)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_title('Accuracy', size=20);\n",
        "\n",
        "test_accuracy = round(model.evaluate(test_data_generator, verbose=0,\n",
        "                          return_dict=True)['accuracy'], 2)\n",
        "training_accuracy = round(history.history['accuracy'][-1], 2)\n",
        "val_accuracy = round(history.history['val_accuracy'][-1], 2)\n",
        "\n",
        "count_params = model.count_params()\n",
        "model_summary = pd.DataFrame({'test_acc':test_accuracy,\n",
        "                              'training_acc': training_accuracy,\n",
        "                              'val_acc': val_accuracy,\n",
        "                              'num_params':  f'{count_params:,}'}, index=[0]) \n",
        "\n",
        "print(model.summary())\n",
        "model_summary"
      ],
      "metadata": {
        "id": "iAgUf5GbSBbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('best_hps.values:\\n', best_hps.values)\n",
        "print('tuner.results_summary(1):\\n', tuner.results_summary())   # hyperparameter values of top 10 models\n",
        "print('tuner.get_best_models()[0].summary():\\n', model.summary())"
      ],
      "metadata": {
        "id": "iLO10XxEfjFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the hyperparameter values of top 10 models \n",
        "# tuner.results_summary()                                 # tuner.results_summary(1) => hyperparameter values of top model"
      ],
      "metadata": {
        "id": "zTTakeLM0-PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "\n",
        "def error_analysis(model_name, model, test_labels, test_data_generator):\n",
        "  num_of_test_samples = len(test_labels)\n",
        "  classes = ['Non-IDC(0)','IDC(1)']\n",
        "\n",
        "  y_true = test_data_generator.classes\n",
        "  y_pred = model.predict_generator(test_data_generator, num_of_test_samples // BATCH_SIZE + 1)    # probabilities\n",
        "  y_pred_argmax = np.argmax(y_pred, axis=1)                      # 0s and 1s: return the indicies of the max values along the axis (axis=1: each row)   TODO: look into prob threshold\n",
        "\n",
        "\n",
        "  # Confusion matrix  \n",
        "  conf_max = confusion_matrix(y_true, y_pred_argmax)             # conf.max should use 0s and 1s for y_pred\n",
        "  perf_conf_max = conf_max.astype('float')/conf_max.sum(axis=1)[:np.newaxis]*100\n",
        "  df_perf_conf_max = pd.DataFrame(perf_conf_max, index=classes, columns=classes)\n",
        "\n",
        "  plt.figure(figsize=(6,5))\n",
        "  sns.heatmap(df_perf_conf_max, annot=True, cmap='coolwarm', annot_kws={'fontsize':16}, linewidth=0.5, fmt='.0f')  \n",
        "  plt.xlabel('Predicted Label', fontsize=14)\n",
        "  plt.ylabel('True Label', fontsize=14)\n",
        "  plt.title('Confusion Matrix (%)', fontsize=15)\n",
        "  \n",
        "  # Classification report (classification_report should use 0s and 1s for y_pred)\n",
        "  print('=============== Classification Report ===============\\n\\n', classification_report(y_true, y_pred_argmax, target_names=['Non-IDC', 'IDC']), '\\n=====================================================\\n')\n",
        "  \n",
        "  # Cross validation score\n",
        "  # cv = ShuffleSplit(n_splits=100, test_size=.25, random_state=0)\n",
        "  # cv_score = np.median(cross_val_score(model, y_true, y_pred, cv=cv))*100\n",
        "  # print('cv_score = ',cv_score)\n",
        "\n",
        "  # Precision, recall, and f1_score\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred_argmax).ravel()     # np.ravel(): returns contiguous flattened array (1D array with all the input-array elements and with the same type as it)\n",
        "  accuracy = round(accuracy_score(y_true, y_pred_argmax), 2)\n",
        "  precision = round(precision_score(y_true, y_pred_argmax), 2)\n",
        "  recall = round(recall_score(y_true, y_pred_argmax), 2)\n",
        "  f1score = round((2*precision*recall)/(precision+recall), 2)\n",
        "  error_rate = round(1-accuracy, 2)\n",
        "\n",
        "  # cohen_kappa score and zero_one loss\n",
        "  cohen_kappa = round(cohen_kappa_score(y_true, y_pred_argmax), 2)\n",
        "  zo_loss = round(zero_one_loss(y_true, y_pred_argmax), 2)\n",
        "\n",
        "  # Area under the ROC curve\n",
        "  roc_log = roc_auc_score(y_true, y_pred[:,1], multi_class='ovr')   # for the roc curve, we need to use a vector of probabilities so just chose one column and all rows\n",
        "  fpr, tpr, threshold = roc_curve(y_true, y_pred[:,1])\n",
        "  area_under_curve = round(auc(fpr, tpr), 2)  \n",
        "\n",
        "  plt.figure(figsize=(6,5))\n",
        "  plt.plot([0, 1], [0, 1], 'r--')  \n",
        "  plt.plot(fpr, tpr, label='ROC-AUC = {:.2f}'.format(area_under_curve))  \n",
        "  plt.xlabel('False positive rate', fontsize=14)\n",
        "  plt.ylabel('True positive rate', fontsize=14)\n",
        "  plt.title('ROC Curve', fontsize=18)\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  model_summary = pd.DataFrame({'Model':model_name,\n",
        "                                'Accuracy': accuracy,\n",
        "                                'Precision': precision,\n",
        "                                'Recall': recall,\n",
        "                                'F1_score': f1score,\n",
        "                                'Error Rate': error_rate,\n",
        "                                'ROC-AUC score': area_under_curve,\n",
        "                                'Cohen Kappa': cohen_kappa,\n",
        "                                'Zero-One Loss': zo_loss}, index=[0]) \n",
        "  return model_summary\n",
        "  \n",
        "error_analysis('CNN', model, test_labels, test_data_generator)"
      ],
      "metadata": {
        "id": "55uBtQIKC7fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # plot my model architecture\n",
        "# tf.keras.utils.plot_model(\n",
        "#     model,\n",
        "#     show_shapes=True,\n",
        "#     show_layer_names=False,\n",
        "#     show_layer_activations=True,\n",
        "# )"
      ],
      "metadata": {
        "id": "bTqd7IJeTaVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install cufflinks\n"
      ],
      "metadata": {
        "id": "9pRgWBFBSV5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "labels=['IDC Malignant', 'IDC Benign']\n",
        "values = [78786, 198738]\n",
        "colors = ['#f5b3ae','#c3f3fa']\n",
        "\n",
        "fig = make_subplots(1,1, specs=[[{'type':'domain'}]],\n",
        "                   subplot_titles=[''])\n",
        "fig.add_trace(go.Pie(labels=labels, values=values, textinfo='label+percent', pull=[0.1, 0], scalegroup='one', marker_colors=colors,\n",
        "                    name='data_split'), 1,1)\n",
        "\n",
        "\n",
        "fig.update_layout(title_text='Data Split Before Sampling')\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Xg1BCUW7Q70U",
        "outputId": "a6ddcb39-33d4-47b3-adf2-573515ccf44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"9b38c8e2-d664-4146-9b17-0fcdc3338f05\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9b38c8e2-d664-4146-9b17-0fcdc3338f05\")) {                    Plotly.newPlot(                        \"9b38c8e2-d664-4146-9b17-0fcdc3338f05\",                        [{\"labels\":[\"IDC Malignant\",\"IDC Benign\"],\"marker\":{\"colors\":[\"#f5b3ae\",\"#c3f3fa\"]},\"name\":\"data_split\",\"pull\":[0.1,0],\"scalegroup\":\"one\",\"textinfo\":\"label+percent\",\"values\":[78786,198738],\"type\":\"pie\",\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]}}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Data Split Before Sampling\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9b38c8e2-d664-4146-9b17-0fcdc3338f05');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuRbO-ntQ7bP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}